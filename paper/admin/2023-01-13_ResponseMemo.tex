\documentclass[fleqn,12pt]{article}
\usepackage{setspace,url,fullpage,latexsym,amsmath,amsthm,amssymb,natbib,graphicx,appendix,float,rotating,caption, subcaption,multirow,longtable,colortbl,graphics,graphicx,enumitem,pdflscape,epstopdf}
\usepackage[pdfstartview=FitH,pdfmenubar=true,pdfborder={0 0 0}]{hyperref}
\usepackage[utf8]{inputenc}


\setlength{\tabcolsep}{3pt}										


\singlespacing

\date{}

\begin{document}
\thispagestyle{empty}
\setcounter{page}{0}

\bibliographystyle{apsr}


\section*{Response Memo on ``A Wiki-based Dataset of Military Operations with Novel Strategic Technologies (MONSTr)''} 
We sincerely thank the reviewers and editor for their insights and comments. Responding to the feedback has greatly improved the manuscript. Accordingly, we have made revisions to accommodate the constructive criticisms detailed below.
\tableofcontents

\clearpage  

\section{Clarify the definition and operationalization of military intervention (R2, R3, \& Editor) \textcolor{red}{ - both}}
Editor: The unit of analysis needs to be outlined precisely. On the absence of a troop size threshold for interventions noted by R3, looking to the definition provided by the older IMI collection may be valuable since it also has no troop threshold and seems consistent with the present project. \\

\noindent 
R2: The structure of the dataset, in particular, the unit of analysis, is hard to understand. The authors state that the unit of analysis is the “military intervention ‘leaf’ node” in Wikipedia. Elsewhere, they imply that “interventions” or “operations” are the unit of analysis. What constitutes an “operation”? Can we really assume that the leaf nodes are all at the operation level? If I understand correctly, what constitutes an observation in the dataset is determined by the somewhat arbitrary level of disaggregation of an event in Wikipedia– i.e., events become disaggregated into multiple leaf nodes as Wikipedia contributors create subentries to existing entries. If so, are the units actually comparable? Do we know that all events that meet the criteria for being an operation are captured in Wikipedia? Is a battle the same as an operation?   When there are operations within operations (e.g., Operation Vigilant Warrior within Operation Southern Watch) – are there observations for the lower-level events and the “parent” event in the dataset? Is there time between these lower-level events that is not captured in any observation in the dataset because it does not have a separate Wikipedia entry? Because the unit of observation in the dataset is so important for determining the usefulness of the data, it is critical that the authors clarify these points. While the authors claim that the dataset “extends breadth by improving the precision” of the unit of analysis (p. 12), I am not convinced of the precision of the units. \\

\noindent
R3: The major issue is that there is no meaningful conceptual or operational definition of an ‘intervention’ or an ‘operation’ that is consistently applied across all cases. More specifically, the authors discuss their criteria on pages 5-6 and begin by comparing their approach to MIPS. MIPS requires “the official deployment of at least 500 regular military personnel to attain immediate term political objectives through action against a foreign adversary” (Sullivan and Koch, 2009, 3). The authors state that they do not include a size threshold for their data. Further, they allow for interventions to have military, rather than political objectives. In addition, they also allow for CIA drone strikes. As a result of the loosening of the definition, it is difficult to say what is an intervention and what is not. It seems that any purposeful application of military or armed force in a foreign territory would count. The authors really need to clarify and state their definition clearly, and demonstrate that it has been consistently applied. \\

\textcolor{red}{Unit of analysis will be discrete military operation, add indicator for leafiest battles within ``operations". Discuss scalability of data for scholars to leverage (like campaigns). We have every operation that falls under the definition of a military intervention, eliminating non-political, accidental, contingent uses of force like finding lost boys in caves. We don't have a troop threshold because we are looking at that operational level, similar to IMI but more disaggregated. We essentially have a version of an events dataset, (news article = Wiki page) so borrow some language from those events datasets. These are events at the level of a military operation existing within our scope conditions consistent with the generally accepted definition of a military intervention. The origination of the original Wiki list stemmed from existing reputable datasets in the first place. This will also help us explain the ``part of" item.}

\textcolor{red}{Kerry will finish this with all of Andres's work on definitions.}

\section{Tighten use of terms (R2 \& R3) \textcolor{red}{- Kerry}}
R2: The term military strategy is used too loosely. At times, for example, the authors use the term “military strategy” when referring to the modes of force used in an operation. However, military strategy is much more than just which weapons platforms deliver firepower. \\

\noindent
R3: The authors also use military ‘interventions’ and military ‘operations’ interchangeably. These are not the same things. \\

\textcolor{red}{Use DoD definitions to justify operations UofA, with recognition that this definition is fuzzy in practice.}

\section{Clarify the data's nested structure (R1, R2, R3, \& Editor) \textcolor{red}{- both}}
Editor: Additionally, the relationships among larger or "parent" operations and sub-operations must be clearly delineated.  Similarly, the relationship between "leaf nodes" and criteria in wikipedia and the data generated must be more fully discussed and explained. \\

\noindent 
R1: I wonder if Wikipedia is sufficiently consistent in its "part of" coding to provide a reliable sense of the broader campaigns within which each use of force is nested. This is probably also something that could be checked by hand once the events are identified. \\

\noindent
R3: The authors also allow for interventions within an intervention. For example, Figure 3 shows the data on the Gulf War and has 26 leaves. As discussed in the paper, this means the US conducted 26 interventions as part of the Gulf War. What is the criteria used for saying something is and is not an intervention in this case? It seems that these 26 interventions could just as easily be 52 or 13. While there is value in distinguishing phases or operations within a broader intervention, there should be clear criteria for doing so. \\

\section{More mindfully compare MONSTr with existing datasets (R2, R3, \& Editor)}
Editor: Multiple reviewers ask for greater care when comparing with existing data sets given variation in conceptual and temporal coverage. \\

\noindent 
R2: The authors claim to “uncover information about nearly every post-1989 military intervention described in existing academic datasets plus 425 additional operations”.  The 425 additional operations claim is misleading: (1) many of the “additional” operations seem to be components of the cases in existing datasets – i.e., battles, campaigns, or military operations conducted as part of the broader wars/military interventions that are the units of analysis in other datasets, rather than interventions that were missed by other coders. The authors do partially acknowledge this on page 13, but their language elsewhere in the document is misleading. \\

\noindent 
There are only 10-15 years of overlap with 2/3 of the comparison datasets and most of the comparison datasets end prior to the conclusion of the wars in Iraq and Afghanistan. How many of these “additional” operations occur after 2014 or are components of these wars? Are there significant military interventions that are completely missing from other major datasets in years both datasets cover? If so, please give examples and explain why this might be the case. \\

\noindent
Moreover, they need to be more careful in their comparisons to other datasets. This dataset has more observations because the observations are more disaggregated. Including a table with the value of key variables (modes of force, start and end dates, duration, geocoded location, days into parent, ally count, etc…) for all the observations in one major “parent” intervention could be helpful to the reader.  \\

\noindent
R3: The lack of clear and consistently applied criteria complicates direct comparisons to other datasets, such as MIP and MIPS, which have very clear conceptual and operational definitions. Yet, the authors make a number of empirical comparisons. For example, “our dataset identifies almost ten times as many events during this period as all existing datasets combined” (p. 2). If I understand correctly, in this comparison the Gulf War counts as 1 intervention, and the 26 interventions within it also count as interventions. It’s just not an appropriate comparison. \\

\textcolor{red}{Shift framing from covering what is missing in other datasets to zooming in and providing more granularity. Acknowledge jagged temporal coverage during our time period (compare apples to apples with a table showing our coverage compared to others using the same dates as their coverage, including which are disaggregations and which are new additions). Emphasize the comparison table in our conference slides to show value.} \\

\noindent 
It is not true to say that there is an “assumption of independence across cases” in other data projects on military intervention (p. 9). To my knowledge, none of the mentioned datasets claim interventions to be independent of one another. It is true that relations between cases may not be recorded, but that does not mean they are assumed to be independent. \textcolor{red}{Kerry} \\

\section{Detail validation checks more thoroughly (R1, R2, R3, \& Editor) \textcolor{red}{- Andres}}
    Editor: Also note that greater detail on your verification checks are required, as R1 and R2 note.  R1 in particular sees value in reporting manual checks to ensure the veracity of the data and we agree.  Since we explicitly asked you to reduce the length of your submission to meet our criteria for special data feature papers, much of this detail may exist in fuller versions of the product. \\
    
    \noindent 
    R1: I would like to know whether they checked the coded events to see if there are duplicates or other problematic observations. The dataset is not so large that checking every event is impossible. Even hand-coded datasets like the MID data sometimes have these kinds of errors. \\
    
    \textcolor{red}{We ensured there are no duplicates because of unique wiki IDs, data validation tools, and manual row-by-row reading of every single row and Wiki page by 4 RAs and Andres. In some cases, we have actual deck logs / after action reports of some of the observations. Work through a specific example or two in the manuscript where we have this. Add a table in the appendix of our validation checks including sources.}\\
    
    \noindent
    R2: The paper should include much more detailed information about how the authors conducted “manual validation” of the values of key variables in the dataset. In particular, the authors need to provide more thorough evidence that Wikipedia’s coding of “means of force used” is accurate, that Wikipedia reliably identifies all battles and operations conducted within a broader campaign/war/intervention, and that the start and end dates for battles and uses of particular means of force are accurate. Which external sources were used to verify the accuracy of Wikipedia’s information? Can you provide statistics on the reliability of Wikipedia coding of the values of key variables? \\
    
    \noindent 
    R3: Would definitely want to see more about how the Wikipedia data was validated. Did the authors use the same sources that were cited for Wiki? They claim data was randomly sampled and checked against external sources (p. 9). What are those external sources? Are they news sources? Government sources? \\

\section{Provide more information on Wiki criteria for inclusion (R3) \textcolor{red}{- Andres}}
The authors could provide a discussion of how a datapoint gets into the RDF databases in the first place (p.7-8). Is there any sort of review required? Documentation? Discussion? If so, what is it? We know a lot about media coverage of conflict, or at least we know something about it (e.g., Croicu and Eck 2022, Dietrich and Eck 2020). How does Wikipedia compare in terms of making the record? \\

\noindent 
In addition, how does something in the RDF get labeled? For example, looking at Figure 3, some are battles and some are operations. What’s the difference? \\

\section{Address what events might be missing given data collection protocols (R1) \textcolor{red}{- Kerry}}
Because the coding rules include no restriction on the minimum size of the event coded, I would like to know more about the events that may be missing from the data for idiosyncratic reasons. For instance, the data include some drone strikes, but apparently not all of them. If there are more inclusive lists of drone strikes--as I believe there are--it would be useful to know how many of these events wound up in this dataset. \\

\textcolor{red}{Things are missing that the public still doesn't know happened on some level.}

\section{Explain the absence of naval interventions (R1) \textcolor{red}{- Andres}}
The authors note on page 16 that "[b]y our definition of intervention, we do not observe any naval interventions within our time scope." I find this statement puzzling. The definition of intervention given on page 5 explicitly includes naval actions. If the data collection procedure somehow did not find any of them, I would like to know why. \\

\textcolor{red}{Look at alleged naval interventions in other datasets. If they code cruise missiles launched from a naval vessel add an indicator for whether the application of force originated from there.}

\section{Justify the absence of parent fixed effects in the demonstrative model (R2)}
R2 observes a discrepancy between 1) our argument that the data enable us to model strategic dependencies between operations and their parent campaigns and 2) the demonstration using the data. We realized from this that we failed to articulate in the manuscript that our models do capture the parental nestings of the 326 operations. Rather than including them as fixed effects in a logit model, we ran hierarchical models that require specification of which variables capture the nested structure. We have corrected this at the end of the first paragraph under the ``Specification" subsection, now reading:

\begin{quote}
    ``The dependent variables are binary and the data are nested, calling for a logit estimator in hierarchical models that account for the strategic dependencies of operations within campaigns (parents) and wars (grandparents)."
\end{quote}

\noindent 
Related to this, R2 wonders how many of the observations in the models took place within Operations Iraqi Freedom and Enduring Freedom. \textcolor{red}{JAG: Add these numbers in the response memo. Explain that we are adding these elements to the dataset so that future scholars can play with these themselves to identify how influential these campaigns are.} \\

\section{Eliminate unhelpful supplements (R2)}
Table 2 and Figure one are not useful because observations are based on different units of analysis in each dataset. \\

\textcolor{red}{Eliminate upset plot. Move table 2 to appendix with verbiage that we are comparing apples to oranges, demonstrating what we gain by a unit of analysis shift to the operations level. Also add a small demo akin to the table in the slides (#16) to show the increase in granularity / dimensionality. Andres will move (there is a placeholder in the appendix) and update table 2 and add example table to appendix. Kerry will do the words part explaining. Kerry will handle response memo items and will remove references from main manuscript.}

\clearpage
\setstretch{1.0}

\end{document}